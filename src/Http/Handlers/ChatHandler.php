<?php
/**
 * èŠå¤©å¤„ç†å™¨
 */

namespace SmartBook\Http\Handlers;

use SmartBook\Http\Context;
use SmartBook\AI\AIService;
use SmartBook\Cache\CacheService;
use SmartBook\RAG\EmbeddingClient;
use SmartBook\RAG\VectorStore;
use SmartBook\AI\TokenCounter;
use SmartBook\Logger;
use Workerman\Protocols\Http\Response;

class ChatHandler
{   
    /**
     * æµå¼ä¹¦ç±é—®ç­”åŠ©æ‰‹ï¼ˆSSEï¼‰
     */
    public static function streamAskAsync(Context $ctx): ?array
    {
        $connection = $ctx->connection();
        $body = $ctx->jsonBody() ?? [];
        $question = $body['question'] ?? '';
        $chatId = $body['chat_id'] ?? '';
        $enableSearch = $body['search'] ?? true;
        $engine = $body['engine'] ?? 'google';
        $ragEnabled = $body['rag'] ?? true;
        $keywordWeight = floatval($body['keyword_weight'] ?? 0.5);
        $model = $body['model'] ?? 'gemini-2.5-flash';
        
        if (empty($question)) return ['error' => 'Missing question'];
        
        $headers = ['Content-Type' => 'text/event-stream', 'Cache-Control' => 'no-cache', 'Access-Control-Allow-Origin' => '*'];
        
        CacheService::getChatContext($chatId, function($context) use ($connection, $question, $chatId, $headers, $enableSearch, $engine, $ragEnabled, $keywordWeight, $model) {
            $connection->send(new Response(200, $headers, ''));
            
            $prompts = $GLOBALS['config']['prompts'];
            $libraryPrompts = $prompts['library'];
            $ragPrompts = $prompts['rag'];
            
            $bookTitle = 'æœªçŸ¥ä¹¦ç±';
            $bookAuthors = 'æœªçŸ¥ä½œè€…';
            
            $currentBookPath = ConfigHandler::getCurrentBookPath();
            if ($currentBookPath) {
                $ext = strtolower(pathinfo($currentBookPath, PATHINFO_EXTENSION));
                if ($ext === 'epub') {
                    $metadata = \SmartBook\Parser\EpubParser::extractMetadata($currentBookPath);
                    if (!empty($metadata['title'])) $bookTitle = 'ã€Š' . $metadata['title'] . 'ã€‹';
                    if (!empty($metadata['authors'])) $bookAuthors = $metadata['authors'];
                } else {
                    $bookTitle = 'ã€Š' . pathinfo($currentBookPath, PATHINFO_FILENAME) . 'ã€‹';
                }
            }
            
            $doChat = function($ragContext, $ragSources) use (
                $connection, $question, $chatId, $enableSearch, $engine, $ragEnabled, $model,
                $context, $bookTitle, $bookAuthors, $prompts, $libraryPrompts, $ragPrompts
            ) {
                if ($ragEnabled && !empty($ragContext)) {
                    $bookInfo = str_replace('{title}', $bookTitle, $ragPrompts['book_intro'] ?? 'I am discussing the book: {title}');
                    if (!empty($bookAuthors)) {
                        $bookInfo .= str_replace('{authors}', $bookAuthors, $ragPrompts['author_template'] ?? ' by {authors}');
                    }
                    $systemPrompt = str_replace(['{book_info}', '{context}'], [$bookInfo, $ragContext], $ragPrompts['system'] ?? 'You are a book analysis assistant. {book_info}\n\nContext:\n{context}');
                    StreamHelper::sendSSE($connection, 'sources', json_encode($ragSources, JSON_UNESCAPED_UNICODE));
                } else {
                    $bookInfo = $libraryPrompts['book_intro'] . str_replace(['{which}', '{title}', '{authors}'], ['', $bookTitle, $bookAuthors], $libraryPrompts['book_template']) . $libraryPrompts['separator'];
                    $systemPrompt = $bookInfo . $libraryPrompts['markdown_instruction'] . ($libraryPrompts['unknown_single'] ?? '') . ' ' . str_replace('{language}', $prompts['language']['default'], $prompts['language']['instruction']);
                    $sourceTexts = $prompts['source_texts'] ?? ['google' => 'AI é¢„è®­ç»ƒçŸ¥è¯† + Google Search', 'mcp' => 'AI é¢„è®­ç»ƒçŸ¥è¯† + MCP å·¥å…·', 'off' => 'AI é¢„è®­ç»ƒçŸ¥è¯†ï¼ˆæœç´¢å·²å…³é—­ï¼‰'];
                    StreamHelper::sendSSE($connection, 'sources', json_encode([['text' => $sourceTexts[$engine] ?? $sourceTexts['off'], 'score' => 100]], JSON_UNESCAPED_UNICODE));
                }
                
                if ($context['summary']) {
                    $historyLabel = $prompts['summarize']['history_label'] ?? 'ã€å¯¹è¯å†å²æ‘˜è¦ã€‘';
                    $systemPrompt .= "\n\n{$historyLabel}\n" . $context['summary']['text'];
                    StreamHelper::sendSSE($connection, 'summary_used', json_encode(['rounds_summarized' => $context['summary']['rounds_summarized'], 'recent_messages' => count($context['messages']) / 2], JSON_UNESCAPED_UNICODE));
                }
                
                $messages = [['role' => 'system', 'content' => $systemPrompt]];
                foreach ($context['messages'] as $msg) {
                    if (isset($msg['role']) && isset($msg['content'])) $messages[] = ['role' => $msg['role'], 'content' => $msg['content']];
                }
                $messages[] = ['role' => 'user', 'content' => $question];
                
                if ($chatId) CacheService::addToChatHistory($chatId, ['role' => 'user', 'content' => $question]);
                
                $asyncGemini = AIService::getAsyncGemini($model);
                $isConnectionAlive = true;
                $requestId = $asyncGemini->chatStreamAsync(
                    $messages,
                    function ($text, $isThought) use ($connection, &$isConnectionAlive, &$requestId, $asyncGemini) {
                        if (!$isConnectionAlive) return;
                        if ($text) {
                            if (!StreamHelper::sendSSE($connection, $isThought ? 'thinking' : 'content', $text)) {
                                $isConnectionAlive = false;
                                if ($requestId) $asyncGemini->cancel($requestId);
                            }
                        }
                    },
                    function ($fullAnswer, $usageMetadata = null, $usedModel = null) use ($connection, $chatId, $context, $model, &$isConnectionAlive) {
                        if (!$isConnectionAlive) return;
                        if ($chatId) {
                            CacheService::addToChatHistory($chatId, ['role' => 'assistant', 'content' => $fullAnswer]);
                            self::triggerSummarizationIfNeeded($chatId, $context);
                        }
                        if ($usageMetadata) {
                            $costInfo = TokenCounter::calculateCost($usageMetadata, $usedModel ?? $model);
                            StreamHelper::sendSSE($connection, 'usage', json_encode(['tokens' => $costInfo['tokens'], 'cost' => $costInfo['cost'], 'cost_formatted' => TokenCounter::formatCost($costInfo['cost']), 'currency' => $costInfo['currency'], 'model' => $usedModel ?? $model], JSON_UNESCAPED_UNICODE));
                        }
                        StreamHelper::sendSSE($connection, 'done', '');
                        $connection->close();
                    },
                    function ($error) use ($connection, &$isConnectionAlive) {
                        if (!$isConnectionAlive) return;
                        StreamHelper::sendSSE($connection, 'error', $error);
                        $connection->close();
                    },
                    ['enableSearch' => $enableSearch && $engine === 'google', 'enableTools' => $engine === 'mcp']
                );
            };
            
            $currentCache = ConfigHandler::getCurrentBookCache();
            if ($ragEnabled && $currentCache) {
                try {
                    $embedder = new EmbeddingClient(GEMINI_API_KEY);
                    $queryEmbedding = $embedder->embedQuery($question);
                    
                    $ragContext = '';
                    $ragSources = [];
                    $chunkTemplate = $ragPrompts['chunk_template'] ?? "ã€Passage {index}ã€‘\n{text}\n";
                    
                    $vectorStore = new VectorStore($currentCache);
                    $results = $vectorStore->hybridSearch($question, $queryEmbedding, 5, $keywordWeight);
                    
                    foreach ($results as $i => $result) {
                        $ragContext .= str_replace(['{index}', '{text}'], [$i + 1, $result['chunk']['text']], $chunkTemplate);
                        $ragContext .= "(Relevance: " . round($result['score'] * 100, 1) . "%)\n\n";
                        $ragSources[] = ['text' => mb_substr($result['chunk']['text'], 0, 200) . '...', 'score' => round($result['score'] * 100, 1)];
                    }
                    $doChat($ragContext, $ragSources);
                } catch (\Exception $e) {
                    $doChat('', []);
                }
            } else {
                $doChat('', []);
            }
        });
        
        return null;
    }
    
    /**
     * æµå¼é€šç”¨èŠå¤©ï¼ˆSSEï¼‰
     */
    public static function streamChat(Context $ctx): ?array
    {
        $connection = $ctx->connection();
        $body = $ctx->jsonBody() ?? [];
        $message = $body['message'] ?? '';
        $chatId = $body['chat_id'] ?? '';
        $enableSearch = $body['search'] ?? true;
        $engine = $body['engine'] ?? 'google';
        $model = $body['model'] ?? 'gemini-2.5-flash';
        
        $clientSummary = $body['summary'] ?? null;
        $clientHistory = $body['history'] ?? null;
        
        if (empty($message)) return ['error' => 'Missing message'];
        
        $headers = ['Content-Type' => 'text/event-stream', 'Cache-Control' => 'no-cache', 'Access-Control-Allow-Origin' => '*'];
        
        if ($clientSummary !== null || $clientHistory !== null) {
            $connection->send(new Response(200, $headers, ''));
            
            $prompts = $GLOBALS['config']['prompts'];
            $sourceTexts = $prompts['source_texts'] ?? ['google' => 'AI é¢„è®­ç»ƒçŸ¥è¯† + Google Search', 'mcp' => 'AI é¢„è®­ç»ƒçŸ¥è¯† + MCP å·¥å…·', 'off' => 'AI é¢„è®­ç»ƒçŸ¥è¯†ï¼ˆæœç´¢å·²å…³é—­ï¼‰'];
            StreamHelper::sendSSE($connection, 'sources', json_encode([['text' => $sourceTexts[$engine] ?? $sourceTexts['off'], 'score' => 100]], JSON_UNESCAPED_UNICODE));
            
            $systemPrompt = $prompts['chat']['system'] ?? '';
            $messages = [['role' => 'system', 'content' => $systemPrompt]];
            
            if ($clientSummary) {
                $historyLabel = $prompts['summarize']['history_label'] ?? 'ã€å¯¹è¯å†å²æ‘˜è¦ã€‘';
                $messages[0]['content'] .= "\n\n{$historyLabel}\n" . $clientSummary;
                StreamHelper::sendSSE($connection, 'summary_used', json_encode(['source' => 'ios_client', 'has_summary' => true], JSON_UNESCAPED_UNICODE));
            }
            
            if (is_array($clientHistory)) {
                foreach ($clientHistory as $msg) {
                    if (isset($msg['role']) && isset($msg['content'])) {
                        $messages[] = ['role' => $msg['role'], 'content' => $msg['content']];
                    }
                }
            }
            
            $messages[] = ['role' => 'user', 'content' => $message];
            
            Logger::info("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”");
            Logger::info("ğŸ“‹ æäº¤ç»™ Gemini çš„å®Œæ•´ Prompt");
            Logger::info("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”");
            Logger::info("ğŸ¤– æ¨¡å‹: {$model}");
            Logger::info("ğŸ“Š æ¶ˆæ¯æ•°é‡: " . count($messages));
            Logger::info("");
            
            foreach ($messages as $index => $msg) {
                $role = match($msg['role']) {
                    'system' => 'âš™ï¸ System',
                    'user' => 'ğŸ‘¤ User',
                    'assistant' => 'ğŸ¤– Assistant',
                    default => 'â“ Unknown'
                };
                
                $content = $msg['content'];
                $length = mb_strlen($content);
                
                Logger::info("[æ¶ˆæ¯ " . ($index + 1) . "] {$role} ({$length} å­—ç¬¦)");
                Logger::info("---");
                Logger::info($content);
                Logger::info("---");
                Logger::info("");
            }
            
            $totalLength = array_reduce($messages, fn($sum, $msg) => $sum + mb_strlen($msg['content']), 0);
            $estimatedTokens = intval($totalLength / 3);
            
            Logger::info("ğŸ“Š ç»Ÿè®¡ä¿¡æ¯:");
            Logger::info("  â€¢ æ€»æ¶ˆæ¯æ•°: " . count($messages));
            Logger::info("  â€¢ æ€»å­—ç¬¦æ•°: {$totalLength}");
            Logger::info("  â€¢ ä¼°ç®— Tokens: ~{$estimatedTokens}");
            Logger::info("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”");
            
            $asyncGemini = AIService::getAsyncGemini($model);
            $isConnectionAlive = true;
            $requestId = $asyncGemini->chatStreamAsync(
                $messages,
                function ($text, $isThought) use ($connection, &$isConnectionAlive, &$requestId, $asyncGemini) {
                    if (!$isConnectionAlive) return;
                    if ($text) {
                        if (!StreamHelper::sendSSE($connection, $isThought ? 'thinking' : 'content', $text)) {
                            $isConnectionAlive = false;
                            if ($requestId) $asyncGemini->cancel($requestId);
                        }
                    }
                },
                function ($fullContent, $usageMetadata = null, $usedModel = null) use ($connection, $model, &$isConnectionAlive) {
                    if (!$isConnectionAlive) return;
                    if ($usageMetadata) {
                        $costInfo = TokenCounter::calculateCost($usageMetadata, $usedModel ?? $model);
                        StreamHelper::sendSSE($connection, 'usage', json_encode([
                            'tokens' => $costInfo['tokens'], 
                            'cost' => $costInfo['cost'], 
                            'cost_formatted' => TokenCounter::formatCost($costInfo['cost']), 
                            'currency' => $costInfo['currency'], 
                            'model' => $usedModel ?? $model
                        ], JSON_UNESCAPED_UNICODE));
                    }
                    StreamHelper::sendSSE($connection, 'done', ''); 
                    $connection->close(); 
                },
                function ($error) use ($connection, &$isConnectionAlive) {
                    if (!$isConnectionAlive) return;
                    StreamHelper::sendSSE($connection, 'error', $error); 
                    $connection->close(); 
                },
                ['enableSearch' => $enableSearch && $engine === 'google', 'enableTools' => $engine === 'mcp']
            );
            
            return null;
        }
        
        CacheService::getChatContext($chatId, function($context) use ($connection, $message, $chatId, $headers, $enableSearch, $engine, $model) {
            $connection->send(new Response(200, $headers, ''));
            
            $prompts = $GLOBALS['config']['prompts'];
            $sourceTexts = $prompts['source_texts'] ?? ['google' => 'AI é¢„è®­ç»ƒçŸ¥è¯† + Google Search', 'mcp' => 'AI é¢„è®­ç»ƒçŸ¥è¯† + MCP å·¥å…·', 'off' => 'AI é¢„è®­ç»ƒçŸ¥è¯†ï¼ˆæœç´¢å·²å…³é—­ï¼‰'];
            StreamHelper::sendSSE($connection, 'sources', json_encode([['text' => $sourceTexts[$engine] ?? $sourceTexts['off'], 'score' => 100]], JSON_UNESCAPED_UNICODE));
            
            $systemPrompt = $prompts['chat']['system'] ?? 'ä½ æ˜¯ä¸€ä¸ªå‹å–„ã€åšå­¦çš„ AI åŠ©æ‰‹ï¼Œæ“…é•¿å›ç­”å„ç§é—®é¢˜å¹¶æä¾›æœ‰ä»·å€¼çš„è§è§£ã€‚è¯·ç”¨ä¸­æ–‡å›ç­”ã€‚';
            $messages = [['role' => 'system', 'content' => $systemPrompt]];
            
            if ($context['summary']) {
                $historyLabel = $prompts['summarize']['history_label'] ?? 'ã€å¯¹è¯å†å²æ‘˜è¦ã€‘';
                $messages[0]['content'] .= "\n\n{$historyLabel}\n" . $context['summary']['text'];
                StreamHelper::sendSSE($connection, 'summary_used', json_encode([
                    'rounds_summarized' => $context['summary']['rounds_summarized'],
                    'recent_messages' => count($context['messages']) / 2
                ], JSON_UNESCAPED_UNICODE));
            }
            
            foreach ($context['messages'] as $msg) {
                if (isset($msg['role']) && isset($msg['content'])) {
                    $messages[] = ['role' => $msg['role'], 'content' => $msg['content']];
                }
            }
            $messages[] = ['role' => 'user', 'content' => $message];
            
            if ($chatId) {
                CacheService::addToChatHistory($chatId, ['role' => 'user', 'content' => $message]);
            }
            
            $asyncGemini = AIService::getAsyncGemini($model);
            $isConnectionAlive = true;
            $requestId = $asyncGemini->chatStreamAsync(
                $messages,
                function ($text, $isThought) use ($connection, &$isConnectionAlive, &$requestId, $asyncGemini) {
                    if (!$isConnectionAlive) return;
                    if ($text) {
                        if (!StreamHelper::sendSSE($connection, $isThought ? 'thinking' : 'content', $text)) {
                            $isConnectionAlive = false;
                            if ($requestId) $asyncGemini->cancel($requestId);
                        }
                    }
                },
                function ($fullContent, $usageMetadata = null, $usedModel = null) use ($connection, $chatId, $context, $model, &$isConnectionAlive) {
                    if (!$isConnectionAlive) return;
                    if ($chatId) {
                        CacheService::addToChatHistory($chatId, ['role' => 'assistant', 'content' => $fullContent]);
                        self::triggerSummarizationIfNeeded($chatId, $context);
                    }
                    if ($usageMetadata) {
                        $costInfo = TokenCounter::calculateCost($usageMetadata, $usedModel ?? $model);
                        StreamHelper::sendSSE($connection, 'usage', json_encode([
                            'tokens' => $costInfo['tokens'], 
                            'cost' => $costInfo['cost'], 
                            'cost_formatted' => TokenCounter::formatCost($costInfo['cost']), 
                            'currency' => $costInfo['currency'], 
                            'model' => $usedModel ?? $model
                        ], JSON_UNESCAPED_UNICODE));
                    }
                    StreamHelper::sendSSE($connection, 'done', ''); 
                    $connection->close(); 
                },
                function ($error) use ($connection, &$isConnectionAlive) {
                    if (!$isConnectionAlive) return;
                    StreamHelper::sendSSE($connection, 'error', $error); 
                    $connection->close(); 
                },
                ['enableSearch' => $enableSearch && $engine === 'google', 'enableTools' => $engine === 'mcp']
            );
        });
        
        return null;
    }
    
    /**
     * æµå¼ç»­å†™å°è¯´ï¼ˆSSEï¼‰
     */
    public static function streamContinue(Context $ctx): ?array
    {
        $connection = $ctx->connection();
        $body = $ctx->jsonBody() ?? [];
        $prompt = $body['prompt'] ?? '';
        $enableSearch = $body['search'] ?? false;
        $engine = $body['engine'] ?? 'off';
        $ragEnabled = $body['rag'] ?? false;
        $keywordWeight = floatval($body['keyword_weight'] ?? 0.5);
        $model = $body['model'] ?? 'gemini-2.5-flash';
        
        $headers = ['Content-Type' => 'text/event-stream', 'Cache-Control' => 'no-cache', 'Access-Control-Allow-Origin' => '*'];
        $connection->send(new Response(200, $headers, ''));
        
        $prompts = $GLOBALS['config']['prompts'];
        $ragPrompts = $prompts['rag'];
        
        $bookTitle = 'å½“å‰ä¹¦ç±';
        $currentBookPath = ConfigHandler::getCurrentBookPath();
        if ($currentBookPath) {
            $ext = strtolower(pathinfo($currentBookPath, PATHINFO_EXTENSION));
            if ($ext === 'epub') {
                $metadata = \SmartBook\Parser\EpubParser::extractMetadata($currentBookPath);
                if (!empty($metadata['title'])) $bookTitle = 'ã€Š' . $metadata['title'] . 'ã€‹';
            } else {
                $bookTitle = 'ã€Š' . pathinfo($currentBookPath, PATHINFO_FILENAME) . 'ã€‹';
            }
        }
        
        $baseSystemPrompt = str_replace('{title}', $bookTitle, $prompts['continue']['system'] ?? '');
        $userPrompt = $prompt ?: str_replace('{title}', $bookTitle, $prompts['continue']['default_prompt'] ?? '');
        
        $continuePrompts = $prompts['continue'];
        $doChat = function($ragContext, $ragSources) use (
            $connection, $baseSystemPrompt, $userPrompt, $enableSearch, $engine, $model, $prompts, $ragEnabled, $continuePrompts
        ) {
            $systemPrompt = $baseSystemPrompt;
            
            if ($ragEnabled && !empty($ragContext)) {
                $ragInstruction = $continuePrompts['rag_instruction'] ?? '';
                $systemPrompt .= str_replace('{context}', $ragContext, $ragInstruction);
                StreamHelper::sendSSE($connection, 'sources', json_encode($ragSources, JSON_UNESCAPED_UNICODE));
            } else {
                $sourceTexts = $prompts['source_texts'] ?? ['google' => 'AI é¢„è®­ç»ƒçŸ¥è¯† + Google Search', 'mcp' => 'AI é¢„è®­ç»ƒçŸ¥è¯† + MCP å·¥å…·', 'off' => 'AI é¢„è®­ç»ƒçŸ¥è¯†ï¼ˆæœç´¢å·²å…³é—­ï¼‰'];
                StreamHelper::sendSSE($connection, 'sources', json_encode([['text' => $sourceTexts[$engine] ?? $sourceTexts['off'], 'score' => 100]], JSON_UNESCAPED_UNICODE));
            }
            
            $asyncGemini = AIService::getAsyncGemini($model);
            $isConnectionAlive = true;
            $requestId = $asyncGemini->chatStreamAsync(
                [['role' => 'system', 'content' => $systemPrompt], ['role' => 'user', 'content' => $userPrompt]],
                function ($text, $isThought) use ($connection, &$isConnectionAlive, &$requestId, $asyncGemini) {
                    if (!$isConnectionAlive) return;
                    if (!$isThought && $text) {
                        if (!StreamHelper::sendSSE($connection, 'content', $text)) {
                            $isConnectionAlive = false;
                            if ($requestId) $asyncGemini->cancel($requestId);
                        }
                    }
                },
                function ($fullContent, $usageMetadata = null, $usedModel = null) use ($connection, $model, &$isConnectionAlive) {
                    if (!$isConnectionAlive) return;
                    if ($usageMetadata) {
                        $costInfo = TokenCounter::calculateCost($usageMetadata, $usedModel ?? $model);
                        StreamHelper::sendSSE($connection, 'usage', json_encode([
                            'tokens' => $costInfo['tokens'], 
                            'cost' => $costInfo['cost'], 
                            'cost_formatted' => TokenCounter::formatCost($costInfo['cost']), 
                            'currency' => $costInfo['currency'], 
                            'model' => $usedModel ?? $model
                        ], JSON_UNESCAPED_UNICODE));
                    }
                    StreamHelper::sendSSE($connection, 'done', ''); 
                    $connection->close(); 
                },
                function ($error) use ($connection, &$isConnectionAlive) {
                    if (!$isConnectionAlive) return;
                    StreamHelper::sendSSE($connection, 'error', $error);
                    $connection->close();
                },
                ['enableSearch' => $enableSearch && $engine === 'google', 'enableTools' => $engine === 'mcp']
            );
        };
        
        $currentCache = ConfigHandler::getCurrentBookCache();
        if ($ragEnabled && $currentCache) {
            try {
                $embedder = new EmbeddingClient(GEMINI_API_KEY);
                $queryEmbedding = $embedder->embedQuery($userPrompt);
                
                $ragContext = '';
                $ragSources = [];
                $chunkTemplate = $ragPrompts['chunk_template'] ?? "ã€Passage {index}ã€‘\n{text}\n";
                
                $vectorStore = new VectorStore($currentCache);
                $results = $vectorStore->hybridSearch($userPrompt, $queryEmbedding, 5, $keywordWeight);
                
                foreach ($results as $i => $result) {
                    $ragContext .= str_replace(['{index}', '{text}'], [$i + 1, $result['chunk']['text']], $chunkTemplate);
                    $ragContext .= "(Relevance: " . round($result['score'] * 100, 1) . "%)\n\n";
                    $ragSources[] = ['text' => mb_substr($result['chunk']['text'], 0, 200) . '...', 'score' => round($result['score'] * 100, 1)];
                }
                $doChat($ragContext, $ragSources);
            } catch (\Exception $e) {
                $doChat('', []);
            }
        } else {
            $doChat('', []);
        }
        
        return null;
    }
    
    /**
     * æ£€æŸ¥å¹¶è§¦å‘ä¸Šä¸‹æ–‡æ‘˜è¦
     */
    public static function triggerSummarizationIfNeeded(string $chatId, array $context): void
    {
        CacheService::needsSummarization($chatId, function($needsSummary) use ($chatId, $context) {
            if (!$needsSummary) return;
            
            CacheService::getChatHistory($chatId, function($history) use ($chatId, $context) {
                if (empty($history)) return;
                
                $prompts = $GLOBALS['config']['prompts'];
                $summarizeConfig = $prompts['summarize'] ?? [];
                $roleNames = $prompts['role_names'] ?? ['user' => 'ç”¨æˆ·', 'assistant' => 'AI'];
                
                $conversationText = "";
                if ($context['summary']) {
                    $prevLabel = $summarizeConfig['previous_summary_label'] ?? 'ã€ä¹‹å‰çš„æ‘˜è¦ã€‘';
                    $newLabel = $summarizeConfig['new_conversation_label'] ?? 'ã€æ–°å¯¹è¯ã€‘';
                    $conversationText .= "{$prevLabel}\n" . $context['summary']['text'] . "\n\n{$newLabel}\n";
                }
                foreach ($history as $msg) {
                    $role = $roleNames[$msg['role']] ?? ($msg['role'] === 'user' ? 'ç”¨æˆ·' : 'AI');
                    $conversationText .= "{$role}: {$msg['content']}\n\n";
                }
                
                $summarizePrompt = CacheService::getSummarizePrompt();
                
                $asyncGemini = AIService::getAsyncGemini();
                $asyncGemini->chatStreamAsync(
                    [
                        ['role' => 'user', 'content' => $conversationText . "\n\n" . $summarizePrompt]
                    ],
                    function ($text, $isThought) { },
                    function ($summaryText) use ($chatId) {
                        if (!empty($summaryText)) {
                            CacheService::saveSummaryAndCompress($chatId, $summaryText);
                            Logger::info("å¯¹è¯ {$chatId} å·²è‡ªåŠ¨æ‘˜è¦");
                        }
                    },
                    function ($error) use ($chatId) {
                        Logger::error("æ‘˜è¦ç”Ÿæˆå¤±è´¥ ({$chatId}): {$error}");
                    },
                    ['enableSearch' => false]
                );
            });
        });
    }
}
